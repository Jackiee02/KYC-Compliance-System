# -*- coding: utf-8 -*-
"""KYC_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uE7a-chixoJSUhMgqfzU5Bo476ixkyV5
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# -*- coding: utf-8 -*-
"""
é¦™æ¸¯ KYC åˆè¦ç³»çµ±
èªªæ˜ï¼š
 - è¨»é‡‹å‡ç‚ºç¹é«”ä¸­æ–‡ï¼ˆé¦™æ¸¯é¢¨æ ¼ï¼‰
 - PDF å ±å‘Šå…§å®¹ç‚ºè‹±æ–‡ï¼ˆç´”æ®µè½ï¼Œä¸å«è¡¨æ ¼ï¼‰
"""

# ------------------------------
# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆColab åŸ·è¡Œæ™‚å¯ç”¨ï¼‰
# ------------------------------
!pip install pandas openpyxl requests datasketch jellyfish fuzzywuzzy python-Levenshtein faker reportlab tqdm rapidfuzz -q

# ------------------------------
# å¥—ä»¶è¼‰å…¥ï¼ˆæ¨™æº–å°å…¥ï¼‰
# ------------------------------
import os, io, re, time, math, json, random, logging, warnings
from functools import lru_cache
from collections import defaultdict
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
import requests
from tqdm.notebook import tqdm

from fuzzywuzzy import fuzz
import jellyfish
from datasketch import MinHash, MinHashLSH

from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle

# PDF å­—å‹è¨»å†Šï¼ˆä¿æŒè‹±æ–‡å­—å‹ç‚ºå…§å»ºå­—å‹ï¼‰
from reportlab.pdfbase import pdfmetrics
# ä¸è¨»å†Šå¤–ä¾†ä¸­æ–‡å­—å‹ä»¥é¿å… OTF/Type1 å•é¡Œï¼ˆPDF ä»¥è‹±æ–‡è¼¸å‡ºï¼‰

# Colab Drive åˆ¤æ–·èˆ‡ mountï¼ˆå¦‚æœåœ¨é Colab ç’°å¢ƒä¹Ÿèƒ½åŸ·è¡Œä½†ä¸ mountï¼‰
try:
    from google.colab import drive, files
    COLAB = True
except Exception:
    COLAB = False

# æ—¥èªŒèˆ‡è­¦å‘Šè¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
warnings.filterwarnings('ignore')

# ------------------------------
# å…¨åŸŸè¨­å®šï¼ˆå¯æŒ‰éœ€èª¿æ•´ï¼‰
# ------------------------------
class Config:
    # æŒ‡å®šè¼¸å‡ºç›®éŒ„ï¼ˆä¾ä½ çš„è¦æ±‚ï¼‰
    OUTPUT_PATH = "/content/drive/MyDrive/Colab Notebooks/KYC"

    # è³‡æ–™è¦æ¨¡
    TOTAL_RECORDS = 15000         # ï¼ˆæ¸¬è©¦æ™‚å¯æ”¹å°ç‚º 200 ç­‰ï¼‰
    REAL_COMPANIES_RATIO = 0.10   # 10% ç‚ºçŸ¥åå…¬å¸æ—ç¾¤

    # GLEIF è¨­å®šï¼ˆæŸ¥è©¢ä¸å—æˆªæ–·ï¼‰
    GLEIF_THRESHOLD = 75          # ä½¿ç”¨ç›¸ä¼¼åº¦é–€æª»ï¼ˆç™¾åˆ†æ¯”ï¼‰
    GLEIF_DELAY = 0.4             # æ¯æ¬¡æŸ¥è©¢å»¶é²ï¼ˆç§’ï¼‰
    GLEIF_MAX_RETRIES = 2

    # OFAC è¨­å®š
    OFAC_SIMILARITY_THRESHOLD = 0.82
    OFAC_INJECT_RATE = 0.005      # æ³¨å…¥å‘½ä¸­æ¯”ä¾‹ï¼ˆæ¸¬è©¦ç”¨ï¼Œ0.005 => 0.5%ï¼‰

    # é¢¨éšªæ¨¡å‹æ¬Šé‡ï¼ˆå¯èª¿ï¼‰
    RISK_WEIGHTS = {
        'lei_missing': 15,
        'kyc_expired': 25,
        'sanctions_hit': 40,
        'high_risk_jurisdiction': 20,
        'large_transaction': 10
    }

    # ä¾è¨»å†Šåœ‹å®¶æ˜ å°„é¢¨éšªç­‰ç´š
    RISK_TIER_MAP = {
        'HK': ('ä¸­', 20),
        'VG': ('é«˜', 50),
        'KY': ('é«˜', 60),
        'CN': ('ä½', 10),
        'SG': ('ä¸­', 25),
        'DEFAULT': ('ä¸­', 30)
    }

    # å»é‡åƒæ•¸ï¼ˆé™ä½é–¾å€¼ä»¥æ›´æ•æ„Ÿï¼‰
    MINHASH_THRESHOLD = 0.60
    MINHASH_PERMUTATIONS = 64

    # å»é‡å‹•æ…‹æ¬Šé‡ï¼ˆé¢¨éšªå°å‘ï¼‰
    DEDUP_BASE_WEIGHTS = {'registration_match':0.45,'country_match':0.35,'name_similarity':0.20}
    DEDUP_RISK_PROFILE_WEIGHTS = {
        'æ¥µé«˜é¢¨éšª': {'registration_match':0.70,'country_match':0.25,'name_similarity':0.05},
        'é«˜':       {'registration_match':0.60,'country_match':0.30,'name_similarity':0.10},
        'ä¸­':       {'registration_match':0.45,'country_match':0.35,'name_similarity':0.20},
        'ä½':       {'registration_match':0.35,'country_match':0.45,'name_similarity':0.20},
        'DEFAULT':  {'registration_match':0.45,'country_match':0.35,'name_similarity':0.20}
    }
    DEDUP_REGNUM_MISSING_WEIGHTS = {'registration_match':0.0,'country_match':0.60,'name_similarity':0.40}
    DEDUP_REGNUM_INVALID_WEIGHTS = {'registration_match':0.20,'country_match':0.40,'name_similarity':0.40}
    DEDUP_SANCTIONS_BOOST = {'registration_match':0.80,'country_match':0.15,'name_similarity':0.05}

    # KYC é€±æœŸï¼ˆéŠ€è¡Œå¸¸è¦‹å€¼ï¼‰
    KYC_PERIODS_DAYS = {'æ¥µé«˜é¢¨éšª':365,'é«˜':730,'ä¸­':1095,'ä½':1825}

    # æª”æ¡ˆåç¨±è¨­å®š
    FILE_RAW = "kyc_raw_data.csv"
    FILE_CLEAN = "kyc_cleaned_data.csv"
    FILE_LEI = "kyc_lei_enhanced_data.csv"
    FILE_DEDUP = "kyc_deduplicated_data.csv"
    FILE_DUP_TEST = "test_environment_duplicates.csv"
    FILE_EXCEL = "KYCåˆè¦å ±å‘Š_è©³ç´°ç‰ˆ.xlsx"
    FILE_PDF = "KYC_Compliance_Summary_Report.pdf"
    FILE_GUIDE = "verification_guide.txt"
    FILE_MANUAL_REVIEW = "manual_review_notes.json"

    # é€²åº¦æ¢é–‹é—œ
    PROGRESS_BAR_ENABLED = True

# å˜—è©¦æ›è¼‰ Google Driveï¼ˆè‹¥åœ¨ Colabï¼‰
if COLAB:
    try:
        drive.mount('/content/drive', force_remount=True)
    except Exception as e:
        logging.warning(f"drive.mount å˜—è©¦ï¼š{e}")
os.makedirs(Config.OUTPUT_PATH, exist_ok=True)
logging.info(f"è¼¸å‡ºç›®éŒ„ï¼š{Config.OUTPUT_PATH}")

# ------------------------------
# å“åæ¸…æ´—èˆ‡ LEI å·¥å…·ï¼ˆKYCToolsï¼‰
# ------------------------------
class KYCTools:
    # å¸¸è¦‹å…¬å¸å¾Œç¶´æ˜ å°„ï¼ˆè‹±æ–‡èˆ‡ä¸­æ–‡ï¼‰
    SUFFIX_MAPPING = {
        'LTD':'LIMITED','LTD.':'LIMITED','LIMITED':'LIMITED',
        'INC':'INCORPORATED','INC.':'INCORPORATED','INCORPORATED':'INCORPORATED',
        'PLC':'PUBLIC LIMITED COMPANY','CO':'COMPANY','CO.':'COMPANY',
        'LLC':'LIMITED LIABILITY COMPANY','GROUP':'GROUP','HOLDINGS':'HOLDINGS',
        'æœ‰é™å…¬å¸':'LIMITED','é›†åœ˜':'GROUP','æ§è‚¡':'HOLDINGS'
    }

    # å¸¸è¦‹ç‰¹æ®Šå­—å…ƒæ›¿æ›æ˜ å°„
    SPECIAL_CHAR_MAP = {'&':' AND ','@':' AT ','/':' ','\\':' ','ï¼Œ':',','ã€‚':'.','ï¼ˆ':'(','ï¼‰':')'}

    @staticmethod
    def fullwidth_to_halfwidth(s: str) -> str:
        """å…¨å½¢è½‰åŠå½¢ï¼ˆç‚ºäº†è™•ç†ä¸­è‹±æ··åˆè¼¸å…¥ï¼‰"""
        result = []
        for ch in s:
            code = ord(ch)
            if 0xFF01 <= code <= 0xFF5E:
                code -= 0xFEE0
            result.append(chr(code))
        return ''.join(result)

    @staticmethod
    def normalize_whitespace(s: str) -> str:
        """å°‡å¤šå€‹ç©ºç™½æ”¶æ–‚ç‚ºå–®ä¸€ç©ºç™½ä¸¦ trim"""
        return re.sub(r'\s+', ' ', s).strip()

    @staticmethod
    def clean_company_name(name: str) -> str:
        """
        æ¸…ç†èˆ‡æ¨™æº–åŒ–å…¬å¸åç¨±ï¼š
         - å…¨å½¢è½‰åŠå½¢ã€æ›¿æ›ç‰¹æ®Šå­—å…ƒ
         - ç§»é™¤ä¸å¿…è¦ç¬¦è™Ÿã€ä¿ç•™ä¸­æ–‡èˆ‡è‹±æ–‡
         - æ¨™æº–åŒ–å¸¸è¦‹å¾Œç¶´ä¸¦ç§»é™¤
         - å›å‚³å¤§å¯«ï¼ˆä¸­æ–‡ä¿ç•™ï¼‰
        """
        if pd.isna(name) or str(name).strip() == "":
            return "MISSING_NAME"
        s = str(name).strip()
        s = KYCTools.fullwidth_to_halfwidth(s)
        for k,v in KYCTools.SPECIAL_CHAR_MAP.items():
            s = s.replace(k, v)
        s = re.sub(r'[\[\]\{\}\|<>~`Â©Â®â„¢\*]', ' ', s)
        s = re.sub(r"[^A-Za-z0-9\u4e00-\u9fff\s\-/()]", " ", s)
        s = KYCTools.normalize_whitespace(s)
        s = s.upper()
        if '/' in s:
            parts = [p.strip() for p in s.split('/') if p.strip()]
            english_candidate = ''
            for p in parts:
                if re.search(r'[A-Z]', p):
                    english_candidate = english_candidate + (' ' + p if english_candidate else p)
            s = english_candidate if english_candidate else parts[0]
        words = s.split()
        cleaned = []
        for w in words:
            w2 = w.strip().strip('.').upper()
            cleaned.append(KYCTools.SUFFIX_MAPPING.get(w2, w2))
        s = ' '.join(cleaned)
        s = KYCTools.normalize_whitespace(s)
        tokens = s.split()
        filtered = [t for t in tokens if t not in set(KYCTools.SUFFIX_MAPPING.values())]
        if filtered:
            s = ' '.join(filtered)
        s = KYCTools.normalize_whitespace(s)
        return s if s else "MISSING_NAME"

    @staticmethod
    @lru_cache(maxsize=20000)
    def calculate_similarity(name1: str, name2: str) -> float:
        """
        è¨ˆç®—å…©å€‹åç¨±ç›¸ä¼¼åº¦ï¼š
         - Jaro-Winkler èˆ‡ token_sort_ratio æ··åˆ
         - å›å‚³ 0~1 æµ®é»
        """
        if not name1 or not name2:
            return 0.0
        try:
            jw = jellyfish.jaro_winkler_similarity(name1, name2)
        except Exception:
            jw = 0.0
        token = fuzz.token_sort_ratio(name1, name2) / 100.0
        return round(0.7 * jw + 0.3 * token, 4)

    @staticmethod
    def lei_iso17442_check(lei: Optional[str]) -> bool:
        """
        åš´æ ¼çš„ ISO 17442 LEI æ ¡é©—ï¼ˆåŒ…å«æª¢æŸ¥ç¢¼è¨ˆç®—ï¼‰
        """
        if not lei or pd.isna(lei):
            return False
        lei = str(lei).strip().upper()
        if len(lei) != 20:
            return False
        body = lei[:18]; check = lei[18:]
        numeric_parts = []
        for ch in body:
            if ch.isdigit():
                numeric_parts.append(ch)
            elif 'A' <= ch <= 'Z':
                numeric_parts.append(str(ord(ch) - 55))
            else:
                return False
        numeric_str = ''.join(numeric_parts) + "00"
        total = 0
        for i in range(0, len(numeric_str), 9):
            part = numeric_str[i:i+9]
            total = (total * (10**len(part)) + int(part)) % 97
        computed = (98 - total) % 97
        try:
            check_int = int(check)
        except:
            return False
        return computed == check_int

    @staticmethod
    def safe_lei_validation(lei: Optional[str]) -> bool:
        """å¯¬é¬†å…¥å£å¾Œæ¥åš´æ ¼æ ¡é©—"""
        if not lei or pd.isna(lei):
            return False
        lei = str(lei).strip().upper()
        if not re.match(r'^[0-9A-Z]{20}$', lei):
            return False
        return KYCTools.lei_iso17442_check(lei)

    @staticmethod
    def generate_registration_number(country: str) -> str:
        """æ ¹æ“šåœ‹å®¶ç”Ÿæˆä¸€å€‹é¡ä¼¼çš„ç™»è¨˜è™Ÿ"""
        if country == 'HK':
            return str(random.randint(1000000, 19999999))
        elif country == 'VG':
            return f"VG{random.randint(100000, 999999)}"
        elif country == 'KY':
            return f"KY{random.randint(100000, 999999)}"
        elif country == 'SG':
            return f"{random.randint(2000,2024)}{random.randint(10000,99999)}"
        else:
            return f"{country}-{random.randint(1000,9999)}"

    @staticmethod
    def generate_valid_lei(prefix: Optional[str] = None) -> Optional[str]:
        """
        è‡ªè¡Œç”Ÿæˆä¸€å€‹åˆæ³•æ ¼å¼ä¸”é€šéæª¢æŸ¥ç¢¼çš„ LEI
        - prefix: å¯æŒ‡å®šå‰ 6 ä½ï¼ˆä¾‹å¦‚ 529900ï¼‰
        - å›å‚³ 20 å­—å…ƒ LEI æˆ– Noneï¼ˆç†è«–ä¸Šæœƒç”ŸæˆæˆåŠŸï¼‰
        """
        if prefix and not re.match(r'^[0-9]{6}$', str(prefix)):
            prefix = None
        if not prefix:
            prefix = random.choice(['529900','254900','969500','213800','377900'])
        rest = ''.join(random.choices('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=12))
        body = prefix + rest  # 18 chars
        numeric_parts = []
        for ch in body:
            if ch.isdigit():
                numeric_parts.append(ch)
            elif 'A' <= ch <= 'Z':
                numeric_parts.append(str(ord(ch) - 55))
            else:
                return None
        numeric_str = ''.join(numeric_parts) + "00"
        total = 0
        for i in range(0, len(numeric_str), 9):
            part = numeric_str[i:i+9]
            total = (total * (10**len(part)) + int(part)) % 97
        computed = (98 - total) % 97
        check = f"{computed:02d}"
        lei = body + check
        if KYCTools.safe_lei_validation(lei):
            return lei
        # fallback loop
        for _ in range(10):
            rest = ''.join(random.choices('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=12))
            body = prefix + rest
            numeric_parts = []
            for ch in body:
                numeric_parts.append(ch if ch.isdigit() else str(ord(ch)-55))
            numeric_str = ''.join(numeric_parts) + "00"
            total = 0
            for i in range(0, len(numeric_str), 9):
                part = numeric_str[i:i+9]
                total = (total * (10**len(part)) + int(part)) % 97
            computed = (98 - total) % 97
            lei = body + f"{computed:02d}"
            if KYCTools.safe_lei_validation(lei):
                return lei
        return None

# ------------------------------
# è¼‰å…¥ OFAC SDN åå–®ç¯„ä¾‹ï¼ˆåŠ é€Ÿæ¸¬è©¦ã€ä¸¦å„²å­˜æœ¬åœ°å‰¯æœ¬ï¼‰
# ------------------------------
def load_sdn_sample(limit=500):
    url = "https://www.treasury.gov/ofac/downloads/sdn.csv"
    try:
        r = requests.get(url, timeout=25)
        text = r.text
        lines = [l for l in text.splitlines() if l and not l.startswith('#')]
        df = pd.read_csv(io.StringIO('\n'.join(lines)), header=None, dtype=str, low_memory=False)
        names = []
        if df.shape[1] >= 2:
            names += df[1].dropna().astype(str).tolist()
        names = [n.strip() for n in names if isinstance(n, str) and n.strip()]
        names = list(dict.fromkeys(names))
        return names[:limit]
    except Exception as e:
        logging.warning(f"ä¸‹è¼‰ SDN å¤±æ•—æˆ–ç¶²è·¯ä¸å¯ç”¨ï¼š{e}")
        return ["CENTRAL BANK OF IRAN","VLADIMIR PUTIN","NORTH KOREA","SYRIAN ARAB REPUBLIC"]

SDN_SAMPLE = load_sdn_sample(limit=500)

# ------------------------------
# STEP 1ï¼šç”¢ç”Ÿæ¸¬è©¦è³‡æ–™ï¼ˆå« 2â€“5% é‡è¤‡ã€OFAC æ³¨å…¥ã€LEI ç”Ÿæˆï¼‰
# ------------------------------
def generate_realistic_dataset(total: int = Config.TOTAL_RECORDS,
                               duplicate_rate_min: float = 0.02,
                               duplicate_rate_max: float = 0.05,
                               ofac_inject_rate: float = Config.OFAC_INJECT_RATE,
                               lei_real_rate: float = 0.75,   # çœŸå¯¦å…¬å¸ä¸­ç”Ÿæˆ LEI çš„æ©Ÿç‡
                               lei_other_rate: float = 0.05): # å…¶ä»–å…¬å¸ç”Ÿæˆ LEI çš„æ©Ÿç‡
    """
    ç”Ÿæˆæ¨¡æ“¬ KYC è³‡æ–™ï¼š
    - æœƒæ³¨å…¥ OFAC åç¨±åˆ°ä¸€éƒ¨åˆ†ç´€éŒ„ï¼ˆä»¥æ¸¬è©¦ OFAC å‘½ä¸­ï¼‰
    - æœƒç”Ÿæˆåˆç†åˆ†å¸ƒçš„ last_kyc_reviewï¼ˆåå‘è¿‘æœŸï¼‰
    - æœƒç‚ºçœŸå¯¦å…¬å¸æä¾›é«˜æ©Ÿç‡çš„ LEIï¼ˆå¯æª¢é©—è¦†è“‹ç‡ï¼‰
    - æœƒåœ¨æœ€å¾Œè£½é€  2-5% çš„è¿‘ä¼¼é‡è¤‡ç´€éŒ„
    """
    from faker import Faker
    Faker.seed(2025); random.seed(2025); np.random.seed(2025)
    fake_en = Faker('en_US')

    data = []
    real_count = int(total * Config.REAL_COMPANIES_RATIO)
    REAL_COMPANY_PAIRS = [
        ("HSBC HOLDINGS PLC","æ»™è±æ§è‚¡æœ‰é™å…¬å¸"),
        ("STANDARD CHARTERED PLC","æ¸£æ‰“é›†åœ˜æœ‰é™å…¬å¸"),
        ("BANK OF CHINA LIMITED","ä¸­åœ‹éŠ€è¡Œè‚¡ä»½æœ‰é™å…¬å¸"),
        ("AIA GROUP LIMITED","å‹é‚¦ä¿éšªæ§è‚¡æœ‰é™å…¬å¸"),
        ("HANG SENG BANK LIMITED","æ’ç”ŸéŠ€è¡Œæœ‰é™å…¬å¸"),
        ("TENCENT HOLDINGS LIMITED","é¨°è¨Šæ§è‚¡æœ‰é™å…¬å¸"),
        ("ALIBABA GROUP HOLDING LIMITED","é˜¿é‡Œå·´å·´é›†åœ˜æ§è‚¡æœ‰é™å…¬å¸"),
        ("PING AN INSURANCE GROUP","ä¸­åœ‹å¹³å®‰ä¿éšªé›†åœ˜")
    ]

    countries = ['HK','VG','KY','CN','SG']
    weights = [0.5,0.2,0.15,0.1,0.05]

    # é é¸ OFAC æ³¨å…¥ç´¢å¼•
    num_ofac = int(total * ofac_inject_rate)
    ofac_indices = set(random.sample(range(total), k=num_ofac)) if num_ofac > 0 else set()

    for i in range(total):
        is_real = (i < real_count)
        # è‹¥ç‚ºæ³¨å…¥ç´¢å¼•ï¼Œç›´æ¥ç”¨ SDN ç¯„ä¾‹åç¨±ä»¥ç¢ºä¿å‘½ä¸­
        if i in ofac_indices:
            sdn_name = SDN_SAMPLE[random.randint(0, len(SDN_SAMPLE)-1)]
            company_name = sdn_name
        else:
            if is_real:
                en, zh = random.choice(REAL_COMPANY_PAIRS)
                company_name = f"{en} / {zh}"
            else:
                base_en = fake_en.company().replace(',', '').replace('.', '')
                suffix = random.choice([" Limited"," Ltd"," Group"," Holdings"," PLC"])
                en_name = f"{base_en}{suffix}"
                first_word = base_en.split()[0] if base_en.split() else ""
                translit_map = {'A':'äº','B':'ç™¾','C':'ä¸­','D':'å¤§','E':'æ˜“','F':'å¯Œ','G':'é‡‘','H':'äº¨','I':'æ„›','J':'æ·','K':'åœ‹'}
                zh_core = translit_map.get(first_word[0].upper(), 'å˜‰') if first_word else 'å˜‰'
                industry = random.choice(["æŠ•è³‡","åœ°ç”¢","é‡‘è","ç§‘æŠ€","è²¿æ˜“","å¯¦æ¥­"])
                company_name = f"{en_name} / {zh_core}{industry}æœ‰é™å…¬å¸"

        country = random.choices(countries, weights=weights, k=1)[0]
        reg_no = KYCTools.generate_registration_number(country)
        aum = round(min(np.random.lognormal(3.5, 1.5), 10000), 2)
        inc_date = fake_en.date_between(start_date=datetime(1990,1,1), end_date=datetime(2024,12,31))

        # last_kyc_reviewï¼šæŒ‡æ•¸åˆ†å¸ƒåå‘è¿‘æœŸ
        days_ago = int(min(np.random.exponential(scale=400), 2500))
        last_review_date = datetime.today().date() - timedelta(days=days_ago)

        # LEI ç”Ÿæˆï¼ˆæé«˜çœŸå¯¦å…¬å¸è¦†è“‹ç‡ï¼‰
        lei = None
        if is_real:
            if random.random() < lei_real_rate:
                lei = KYCTools.generate_valid_lei()
        else:
            if random.random() < lei_other_rate:
                lei = KYCTools.generate_valid_lei()

        rec = {
            'record_id': f"KYC{str(i+1).zfill(6)}",
            'company_name': company_name,
            'registered_country': country,
            'reg_no': reg_no,
            'lei': lei,
            'aum_usd_millions': aum,
            'incorporation_date': pd.to_datetime(inc_date),
            'last_kyc_review': pd.to_datetime(last_review_date),
            'is_real_company': is_real,
            'data_source': 'generated'
        }
        data.append(rec)

    df = pd.DataFrame(data)

    # è£½é€  2-5% çš„è¿‘ä¼¼é‡è¤‡ï¼ˆåŠ ä¸Šå°å¹…å™ªéŸ³ï¼‰
    dup_rate = random.uniform(duplicate_rate_min, duplicate_rate_max)
    num_dup = int(total * dup_rate)
    if num_dup > 0:
        dup_indices = random.sample(list(df.index), k=num_dup)
        duplicates = []
        for orig_idx in dup_indices:
            orig = df.loc[orig_idx].to_dict()
            cname = orig['company_name']
            if '/' in cname:
                left, right = [p.strip() for p in cname.split('/')[:2]]
                if random.random() < 0.5:
                    left = left + " "
                else:
                    left = left.lower()
                new_name = f"{left} / {right}"
            else:
                new_name = cname + " "
            rno = orig['reg_no']
            rno_new = rno
            if isinstance(rno, str) and re.search(r'\d', rno):
                last_digits = re.findall(r'\d+', rno)
                if last_digits:
                    last = last_digits[-1]
                    try:
                        new_last = str((int(last[-1]) + 1) % 10)
                        rno_new = re.sub(last+'$', last[:-1]+new_last if len(last)>1 else new_last, rno)
                    except:
                        rno_new = rno
            lr = orig['last_kyc_review']
            if pd.isna(lr):
                lr_new = lr
            else:
                lr_new = pd.to_datetime(lr) + pd.to_timedelta(random.randint(-30,30), unit='d')
            new_rec = orig.copy()
            new_rec['record_id'] = f"KYC_DUP_{orig_idx}"
            new_rec['company_name'] = new_name
            new_rec['reg_no'] = rno_new
            new_rec['last_kyc_review'] = lr_new
            duplicates.append(new_rec)
        if duplicates:
            df = pd.concat([df, pd.DataFrame(duplicates)], ignore_index=True).reset_index(drop=True)

    # å„²å­˜åŸå§‹è³‡æ–™
    raw_path = os.path.join(Config.OUTPUT_PATH, Config.FILE_RAW)
    df.to_csv(raw_path, index=False, encoding='utf-8-sig')
    logging.info(f"Generated {len(df)} records (duplicates: {num_dup}, OFAC-injected: {len(ofac_indices)}) -> saved to {raw_path}")
    return df

# ç”¢ç”Ÿè³‡æ–™ï¼ˆåŸ·è¡Œæ­¤è¡Œæœƒè€—è²»æ™‚é–“ï¼Œæ¸¬è©¦è«‹å…ˆæŠŠ Config.TOTAL_RECORDS æ”¹å°ï¼‰
raw_df = generate_realistic_dataset()

# ------------------------------
# STEP 2ï¼šæ¸…æ´—èˆ‡æ¨™æº–åŒ–
# ------------------------------
def clean_and_standardize(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—æ­¥é©Ÿï¼š
     - company_name æ¸…ç†
     - æ‹†è‹±ä¸­ï¼ˆè‹¥æœ‰ï¼‰
     - reg_no æ­£è¦åŒ–
     - é¢¨éšª tier èˆ‡åŸºç¤åˆ†æ•¸
     - åˆ¤å®š KYC æ˜¯å¦åˆ°æœŸï¼ˆä¾ Config.KYC_PERIODS_DAYSï¼‰
    """
    df2 = df.copy()
    df2['company_name_clean'] = df2['company_name'].apply(KYCTools.clean_company_name)

    def split_bilingual(s):
        if pd.isna(s): return ("","")
        if '/' in s:
            parts = [p.strip() for p in s.split('/') if p.strip()]
            en = parts[0] if re.search(r'[A-Za-z]', parts[0]) else ''
            zh = parts[1] if len(parts) > 1 else ''
            return en, zh
        en = re.sub(r'[\u4e00-\u9fff]+', ' ', str(s)).strip()
        zh = re.sub(r'[A-Za-z0-9\W_]+', ' ', str(s)).strip()
        return en, zh

    df2[['company_name_en','company_name_zh']] = df2['company_name'].apply(lambda x: pd.Series(split_bilingual(x)))
    df2['reg_no_normalized'] = df2['reg_no'].astype(str).str.upper().str.replace(r'[\s\-/]', '', regex=True)
    df2['risk_tier'] = df2['registered_country'].apply(lambda x: Config.RISK_TIER_MAP.get(x, Config.RISK_TIER_MAP['DEFAULT'])[0])
    df2['base_risk_score'] = df2['registered_country'].apply(lambda x: Config.RISK_TIER_MAP.get(x, Config.RISK_TIER_MAP['DEFAULT'])[1])

    today = datetime.today().date()
    def kyc_due(row):
        last = row['last_kyc_review']
        try:
            lastd = pd.to_datetime(last).date()
        except:
            return True
        days_allowed = Config.KYC_PERIODS_DAYS.get(row['risk_tier'], 1095)
        return (today - lastd).days > days_allowed

    df2['kyc_refresh_due'] = df2.apply(kyc_due, axis=1)

    def days_to_next(row):
        try:
            lastd = pd.to_datetime(row['last_kyc_review']).date()
        except:
            return 0
        days_allowed = Config.KYC_PERIODS_DAYS.get(row['risk_tier'], 1095)
        rem = days_allowed - (today - lastd).days
        return max(0, rem)

    df2['days_until_next_review'] = df2.apply(days_to_next, axis=1)
    df2['large_transaction_flag'] = df2['aum_usd_millions'] > 500
    df2['lei_valid'] = df2['lei'].apply(lambda x: KYCTools.safe_lei_validation(x) if pd.notna(x) else False)

    clean_path = os.path.join(Config.OUTPUT_PATH, Config.FILE_CLEAN)
    df2.to_csv(clean_path, index=False, encoding='utf-8-sig')
    logging.info(f"Cleaned data saved: {clean_path}")
    return df2

cleaned_df = clean_and_standardize(raw_df)

# ------------------------------
# STEP 3ï¼šGLEIF LEI å¢å¼·ï¼ˆä¸é™åˆ¶æŸ¥è©¢æ•¸é‡ï¼Œæœƒå˜—è©¦å°æ¯å€‹ candidate æŸ¥è©¢ï¼‰
# ------------------------------
class GLEIFClient:
    """ç°¡æ˜“ GLEIF å®¢æˆ¶ç«¯ï¼›å…·å‚™é‡è©¦èˆ‡ç°¡åŒ–æŸ¥è©¢é‚è¼¯"""
    def __init__(self, base_url="https://api.gleif.org/api/v1/lei-records"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({'User-Agent':'KYC-System-v16','Accept':'application/vnd.api+json'})
        self.cache = {}

    def fetch_lei(self, name: str, max_retries:int = Config.GLEIF_MAX_RETRIES) -> Optional[str]:
        key = name.strip().upper()
        if not key:
            return None
        if key in self.cache:
            return self.cache[key]
        for attempt in range(max_retries+1):
            try:
                params = {"filter[entity.legalName]": name, "page[size]":1}
                resp = self.session.get(self.base_url, params=params, timeout=15)
                if resp.status_code == 200:
                    j = resp.json()
                    if j.get('data'):
                        lei = j['data'][0].get('attributes', {}).get('lei')
                        if lei and KYCTools.safe_lei_validation(lei):
                            self.cache[key] = lei
                            return lei
                # å˜—è©¦ç°¡åŒ–åç¨±ï¼ˆå»æ‰å¸¸è¦‹ legal suffixï¼‰
                if attempt == 0:
                    simplified = re.sub(r'\b(LIMITED|LTD|PLC|INC|INCORPORATED|CORPORATION|GROUP|HOLDINGS)\b','', name, flags=re.IGNORECASE).strip()
                    if simplified and simplified != name:
                        params["filter[entity.legalName]"] = simplified
                        resp = self.session.get(self.base_url, params=params, timeout=10)
                        if resp.status_code == 200:
                            j = resp.json()
                            if j.get('data'):
                                lei = j['data'][0].get('attributes',{}).get('lei')
                                if lei and KYCTools.safe_lei_validation(lei):
                                    self.cache[key] = lei
                                    return lei
            except Exception as e:
                logging.warning(f"GLEIF attempt {attempt} error: {e}")
                time.sleep(1 + attempt)
        self.cache[key] = None
        return None

    def batch_fetch_lei(self, names: List[str]) -> Dict[str,str]:
        out = {}
        iterator = tqdm(names, desc="GLEIF æŸ¥è©¢", leave=False) if Config.PROGRESS_BAR_ENABLED else names
        for n in iterator:
            if not n or len(n) < 3:
                continue
            key = n.strip().upper()
            if key in self.cache and self.cache[key] is not None:
                out[n] = self.cache[key]
                continue
            lei = self.fetch_lei(n)
            if lei:
                out[n] = lei
            time.sleep(Config.GLEIF_DELAY)
        return out

def enhance_lei_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    å°æ‰€æœ‰ candidate åŸ·è¡Œ GLEIF æŸ¥è©¢ï¼ˆä¸å†é™åˆ¶æ•¸é‡ï¼‰
    å„ªå…ˆä¿ç•™åŸå§‹ leiï¼ˆè‹¥å­˜åœ¨ä¸”é©—è­‰é€šéï¼‰ï¼Œå¦å‰‡å¡«å……æŸ¥è©¢åˆ°çš„ lei
    """
    logging.info("é–‹å§‹ LEI å¢å¼·ï¼ˆå˜—è©¦å¤§é‡ GLEIF æŸ¥è©¢ï¼‰")
    client = GLEIFClient()
    candidates = df['company_name_clean'].dropna().unique().tolist()
    candidates = [c for c in candidates if len(c) > 3]
    try:
        lei_map = client.batch_fetch_lei(candidates)
    except Exception as e:
        logging.warning(f"GLEIF batch æŸ¥è©¢å¤±æ•—ï¼š{e}")
        lei_map = {}
    df2 = df.copy()
    def upd(row):
        if pd.notna(row.get('lei')) and KYCTools.safe_lei_validation(row.get('lei')):
            return row.get('lei')
        name = row.get('company_name_clean','')
        if name in lei_map and lei_map[name]:
            return lei_map[name]
        # è¿‘ä¼¼æ¯”å°
        best = None; bests = 0.0
        for k,v in lei_map.items():
            s = KYCTools.calculate_similarity(name, k)
            if s > bests and s >= (Config.GLEIF_THRESHOLD / 100.0):
                bests = s; best = v
        return best
    df2['lei_enhanced'] = df2.apply(upd, axis=1)
    df2['lei_enhanced_valid'] = df2['lei_enhanced'].apply(lambda x: KYCTools.safe_lei_validation(x) if pd.notna(x) else False)
    lei_path = os.path.join(Config.OUTPUT_PATH, Config.FILE_LEI)
    df2.to_csv(lei_path, index=False, encoding='utf-8-sig')
    logging.info(f"LEI å¢å¼·çµæœå·²å„²å­˜ï¼š{lei_path}")
    return df2

enhanced_df = enhance_lei_data(cleaned_df)

# ------------------------------
# STEP 4ï¼šOFAC ç¯©æŸ¥ï¼ˆè§£ææœ¬åœ°æˆ–ç·šä¸Š SDNï¼Œä¸¦è¨ˆç®—ç›¸ä¼¼åº¦ï¼‰
# ------------------------------
class OFACScreener:
    """
    OFAC ç¯©æŸ¥å™¨ï¼š
     - è‹¥æœ‰æœ¬åœ° sdn.csv å‰‡ä½¿ç”¨ï¼Œå¦å‰‡å˜—è©¦ä¸‹è¼‰ä¸¦æœ¬åœ°å„²å­˜
     - æœƒå»ºç«‹ç°¡å–®å­—è©ç´¢å¼•ä»¥åŠ é€Ÿæ¯”å°
    """
    def __init__(self):
        local = os.path.join(Config.OUTPUT_PATH, "sdn.csv")
        lines = []
        if os.path.exists(local):
            logging.info(f"ä½¿ç”¨æœ¬åœ° SDNï¼š{local}")
            with open(local, 'r', encoding='utf-8', errors='ignore') as f:
                lines = [l for l in f.read().splitlines() if l and not l.startswith('#')]
        else:
            try:
                logging.info("ä¸‹è¼‰ OFAC SDN åˆ—è¡¨ï¼ˆä¸¦å„²å­˜æœ¬åœ°å‰¯æœ¬ï¼‰...")
                r = requests.get("https://www.treasury.gov/ofac/downloads/sdn.csv", timeout=30)
                text = r.text
                lines = [l for l in text.splitlines() if l and not l.startswith('#')]
                with open(local, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(lines))
            except Exception as e:
                logging.warning(f"OFAC ä¸‹è¼‰å¤±æ•—ï¼š{e}")
                lines = []

        self.entities = self.parse_sdn_lines(lines)
        self.index = self.build_index(self.entities)

    def parse_sdn_lines(self, lines: List[str]) -> List[str]:
        entities = []
        if not lines:
            return ["CENTRAL BANK OF IRAN","VLADIMIR PUTIN","NORTH KOREA","SYRIAN ARAB REPUBLIC"]
        try:
            df = pd.read_csv(io.StringIO('\n'.join(lines)), header=None, dtype=str, low_memory=False)
            if df.shape[1] >= 2:
                entities += df[1].dropna().astype(str).tolist()
            if df.shape[1] >= 12:
                remarks = df[11].dropna().astype(str).tolist()
                for r in remarks:
                    m = re.findall(r'["\']([^"\']+)["\']', r)
                    if m:
                        entities.extend(m)
                    else:
                        parts = re.split(r'[;,]', r)
                        for p in parts:
                            p = p.strip()
                            if len(p) > 2:
                                entities.append(p)
            ents = list({e.strip().upper() for e in entities if isinstance(e, str) and e.strip()})
            logging.info(f"è§£æ OFAC å¯¦é«”æ•¸é‡ï¼š{len(ents)}")
            return ents
        except Exception as e:
            logging.warning(f"è§£æ SDN CSV å¤±æ•—ï¼š{e}")
            return list({l.strip().upper() for l in lines if l and l.strip()})

    def build_index(self, names: List[str]) -> Dict[str,List[str]]:
        idx = defaultdict(list)
        for n in names:
            for w in set(n.split()):
                if len(w) >= 4:
                    idx[w].append(n)
        return idx

    def screen_company(self, company_name: str) -> Tuple[bool, float, str]:
        """
        å°å–®ä¸€å…¬å¸åç¨±åŸ·è¡Œ OFAC æ¯”å°ï¼Œå›å‚³ (hit:bool, confidence:float, matched_entity:str)
        """
        if not company_name or company_name == "MISSING_NAME":
            return False, 0.0, ""
        s = company_name.upper()
        trans_map = {'IRAN':['ä¼Šæœ—','ä¾æœ—'],'PUTIN':['æ™®äº¬'],'NORTH KOREA':['æœé®®','åŒ—éŸ“']}
        for k, variants in trans_map.items():
            for v in variants:
                if v.upper() in s or s in v.upper():
                    return True, 0.95, k
                if KYCTools.calculate_similarity(s, v.upper()) >= 0.85:
                    return True, KYCTools.calculate_similarity(s, v.upper()), k
        cand = set()
        for w in set(s.split()):
            if len(w) >= 4 and w in self.index:
                cand.update(self.index[w])
        if not cand:
            cand = set(self.entities)
        best_s = 0.0; best_ent = ""
        for ent in cand:
            score = KYCTools.calculate_similarity(s, ent.upper())
            if score > best_s:
                best_s = score; best_ent = ent
        hit = best_s >= Config.OFAC_SIMILARITY_THRESHOLD
        confidence = min(0.99, best_s * 1.1) if hit else max(0.01, best_s * 0.8)
        return bool(hit), round(confidence, 3), (best_ent if hit else "")

def perform_ofac_screening(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("åŸ·è¡Œ OFAC ç¯©æŸ¥...")
    screener = OFACScreener()
    rows = []
    pbar = tqdm(total=len(df), desc="OFAC ç¯©æŸ¥") if Config.PROGRESS_BAR_ENABLED else None
    for idx, row in df.iterrows():
        hit, conf, matched = screener.screen_company(row['company_name_clean'])
        rows.append({'ofac_hit': hit, 'ofac_confidence': conf, 'ofac_matched_entity': matched if hit else ""})
        if pbar: pbar.update(1)
    if pbar: pbar.close()
    ofac_df = pd.DataFrame(rows, index=df.index)
    out = pd.concat([df, ofac_df], axis=1)
    logging.info(f"OFAC ç¯©æŸ¥å®Œæˆã€‚å‘½ä¸­æ•¸ï¼š{int(out['ofac_hit'].sum())}")
    return out

screened_df = perform_ofac_screening(enhanced_df)

# ------------------------------
# STEP 5ï¼šé¢¨éšªè©•ä¼°
# ------------------------------
class RiskAssessor:
    """è¨ˆç®—é¢¨éšªå› å­èˆ‡åˆæˆåˆ†æ•¸"""
    def __init__(self):
        self.weights = Config.RISK_WEIGHTS

    def calculate_factors(self, row):
        base = row.get('base_risk_score', 30) or 30
        lei_risk = self.weights['lei_missing'] if not row.get('lei_enhanced_valid', False) else 0
        kyc_risk = self.weights['kyc_expired'] if row.get('kyc_refresh_due', False) else 0
        sanctions_risk = self.weights['sanctions_hit'] if row.get('ofac_hit', False) else 0
        jurisdiction_risk = self.weights['high_risk_jurisdiction'] if row.get('risk_tier') in ['é«˜','æ¥µé«˜é¢¨éšª'] else 0
        transaction_risk = self.weights['large_transaction'] if row.get('large_transaction_flag', False) else 0
        confidence_penalty = row.get('ofac_confidence', 0) * 10 if row.get('ofac_hit', False) else 0
        return {'base_risk': base, 'lei_risk': lei_risk, 'kyc_risk': kyc_risk, 'sanctions_risk': sanctions_risk, 'jurisdiction_risk': jurisdiction_risk, 'transaction_risk': transaction_risk, 'confidence_penalty': confidence_penalty}

    def composite_score(self, factors):
        total = sum(factors.values())
        try:
            score = 100 * (1 - 1/(1 + math.exp(0.08*(total - 50))))
        except:
            score = min(100, total)
        return round(max(0, min(100, score)), 1)

    def category(self, score):
        if score >= 80: return 'æ¥µé«˜é¢¨éšª', 'ğŸ”´'
        if score >= 60: return 'é«˜', 'ğŸŸ '
        if score >= 40: return 'ä¸­', 'ğŸŸ¡'
        return 'ä½', 'ğŸŸ¢'

    def assess(self, row):
        f = self.calculate_factors(row)
        s = self.composite_score(f)
        cat, emoji = self.category(s)
        primary = max(f.items(), key=lambda x: x[1])[0] if any(f.values()) else 'base_risk'
        return {'risk_score': s, 'risk_category': cat, 'risk_emoji': emoji, 'risk_factors': f, 'primary_risk_factor': primary}

def perform_risk_assessment(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("åŸ·è¡Œé¢¨éšªè©•ä¼°...")
    assessor = RiskAssessor()
    rows = []
    pbar = tqdm(total=len(df), desc="é¢¨éšªè©•ä¼°") if Config.PROGRESS_BAR_ENABLED else None
    for idx, r in df.iterrows():
        rows.append(assessor.assess(r))
        if pbar: pbar.update(1)
    if pbar: pbar.close()
    riskdf = pd.DataFrame(rows, index=df.index)
    out = pd.concat([df, riskdf], axis=1)
    logging.info("é¢¨éšªè©•ä¼°å®Œæˆã€‚")
    return out

assessed_df = perform_risk_assessment(screened_df)

# ------------------------------
# STEP 6ï¼šæ™ºèƒ½å»é‡ï¼ˆå‹•æ…‹æ¬Šé‡ã€MinHash + LSHï¼‰
# ------------------------------
class DeduplicationEngine:
    """å‹•æ…‹æ¬Šé‡å»é‡å¼•æ“ï¼šå…ˆåšå®Œå…¨é‡è¤‡ç§»é™¤ï¼Œå†ç”¨ MinHashLSH æ‰¾æ¨¡ç³Šé‡è¤‡"""
    def __init__(self, threshold=Config.MINHASH_THRESHOLD, num_perm=Config.MINHASH_PERMUTATIONS):
        self.threshold = threshold
        self.num_perm = num_perm
        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
        self.minhash_map = {}

    def create_minhash(self, text: str):
        m = MinHash(num_perm=self.num_perm)
        tokens = text.split()
        for t in tokens:
            m.update(t.encode('utf-8'))
            if len(t) >= 3:
                for i in range(len(t)-2):
                    m.update(t[i:i+3].encode('utf-8'))
        return m

    def compute_dynamic_weights(self, row):
        rc = row.get('risk_category') or row.get('risk_tier') or 'DEFAULT'
        risk_key = rc if rc in Config.DEDUP_RISK_PROFILE_WEIGHTS else 'DEFAULT'
        weights = Config.DEDUP_RISK_PROFILE_WEIGHTS.get(risk_key, Config.DEDUP_BASE_WEIGHTS).copy()
        reg = (row.get('reg_no_normalized') or '').strip()
        reg_exists = bool(reg and reg.upper() not in ['NONE','NAN',''])
        if not reg_exists:
            if risk_key == 'ä½':
                weights = Config.DEDUP_REGNUM_MISSING_WEIGHTS.copy()
            else:
                return None
        else:
            if not re.match(r'^[A-Z0-9\-]{3,}$', reg):
                weights = Config.DEDUP_REGNUM_INVALID_WEIGHTS.copy()
        if row.get('ofac_hit', False):
            weights = Config.DEDUP_SANCTIONS_BOOST.copy()
        s = sum(weights.values())
        if s > 0:
            for k in weights:
                weights[k] = weights[k] / s
        return weights

    def calculate_similarity(self, r1, r2):
        reg1 = (r1.get('reg_no_normalized') or '').upper()
        reg2 = (r2.get('reg_no_normalized') or '').upper()
        reg_match = 1.0 if (reg1 and reg2 and reg1 == reg2) else 0.0
        country_match = 1.0 if (r1.get('registered_country') == r2.get('registered_country')) else 0.0
        name_sim = KYCTools.calculate_similarity(r1.get('company_name_clean',''), r2.get('company_name_clean',''))
        w1 = self.compute_dynamic_weights(r1); w2 = self.compute_dynamic_weights(r2)
        if w1 is None or w2 is None:
            w = {'registration_match':0.0,'country_match':0.6,'name_similarity':0.4}
        else:
            w = {k:(w1.get(k,0) + w2.get(k,0))/2.0 for k in ['registration_match','country_match','name_similarity']}
        combined = reg_match * w['registration_match'] + country_match * w['country_match'] + name_sim * w['name_similarity']
        return combined

    def find_duplicates(self, df: pd.DataFrame, key_columns: List[str]):
        # Step1: å®Œå…¨ç›¸åŒéµå€¼ç§»é™¤ï¼ˆexact dedupï¼‰
        df2 = df.copy()
        exact_mask = df2.duplicated(subset=key_columns, keep='first')
        removed_exact = exact_mask.sum()
        df_exact = df2[~exact_mask].copy()

        # Step2: å»ºç«‹ LSH ä¸¦æœå°‹ç›¸ä¼¼ç¾¤çµ„
        self.lsh = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)
        self.minhash_map = {}
        index_map = {}
        for idx, row in df_exact.iterrows():
            combined = f"{row.get('company_name_clean','')} {row.get('registered_country','')} {row.get('reg_no_normalized','')}"
            mh = self.create_minhash(combined)
            sid = str(idx)
            try:
                self.lsh.insert(sid, mh)
            except Exception:
                pass
            self.minhash_map[sid] = mh
            index_map[sid] = idx

        processed = set()
        groups = []
        pbar = tqdm(total=len(df_exact), desc="æŸ¥æ‰¾ç›¸ä¼¼è¨˜éŒ„") if Config.PROGRESS_BAR_ENABLED else None
        for sid, orig_idx in index_map.items():
            if orig_idx in processed:
                if pbar: pbar.update(1)
                continue
            mh = self.minhash_map[sid]
            try: self.lsh.remove(sid)
            except: pass
            candidate_keys = self.lsh.query(mh)
            try: self.lsh.insert(sid, mh)
            except: pass
            candidate_idxs = [index_map[k] for k in candidate_keys if k in index_map and index_map[k] != orig_idx]
            valid_similar = []
            for cidx in candidate_idxs:
                sim_score = self.calculate_similarity(df_exact.loc[orig_idx], df_exact.loc[cidx])
                if sim_score >= self.threshold:
                    valid_similar.append((cidx, sim_score))
            if valid_similar:
                group = [orig_idx] + [g[0] for g in valid_similar]
                groups.append({'group': group, 'scores': [g[1] for g in valid_similar]})
                processed.update(group)
            processed.add(orig_idx)
            if pbar: pbar.update(1)
        if pbar: pbar.close()

        # Step3: æ±ºå®šä¿ç•™å“ªç­†ï¼ˆä»¥ OFAC å‘½ä¸­æˆ–æœ€é«˜ risk_score ä¿ç•™ï¼‰
        to_remove = set(); manual_review = []
        for g in groups:
            group = g['group']
            if len(group) <= 1:
                continue
            sub = df_exact.loc[group]
            sanc = sub[sub['ofac_hit'] == True] if 'ofac_hit' in sub.columns else pd.DataFrame()
            if not sanc.empty:
                keep_idx = sanc['risk_score'].idxmax()
            else:
                keep_idx = sub['risk_score'].idxmax()
            for rid in group:
                if rid != keep_idx:
                    to_remove.add(rid)
            if max(g['scores']) > 0.98:
                manual_review.append({'primary': keep_idx, 'duplicates': [r for r in group if r != keep_idx], 'max_sim': max(g['scores'])})

        df_final = df_exact.drop(index=list(to_remove)).reset_index(drop=True)

        # åŒ¯å‡ºæ¸¬è©¦å»é‡æ¸…å–®ï¼ˆè‹¥æœ‰ï¼‰
        dup_test_path = os.path.join(Config.OUTPUT_PATH, Config.FILE_DUP_TEST)
        test_rows = []
        for item in groups:
            grp = item['group']
            sub = df_exact.loc[grp]
            keep_idx = sub['risk_score'].idxmax()
            for rem in grp:
                if rem != keep_idx:
                    test_rows.append({'kept_record': int(keep_idx), 'removed_record': int(rem), 'group_size': len(grp)})
        if test_rows:
            pd.DataFrame(test_rows).to_csv(dup_test_path, index=False, encoding='utf-8-sig')
            logging.info(f"ç™¼ç¾é‡è¤‡ï¼Œå·²è¼¸å‡ºæ¸¬è©¦æª”ï¼š{dup_test_path}")
        else:
            if os.path.exists(dup_test_path):
                try: os.remove(dup_test_path)
                except: pass

        logging.info(f"å»é‡å®Œæˆï¼šç²¾ç¢ºç§»é™¤ {removed_exact}ï¼Œæ¨¡ç³Šç§»é™¤ {len(to_remove)}ï¼Œæœ€çµ‚ {len(df_final)}")
        return df_final, manual_review

def perform_deduplication(df: pd.DataFrame):
    deduper = DeduplicationEngine()
    key_cols = ['company_name_clean','registered_country','reg_no_normalized']
    df_final, manual_review = deduper.find_duplicates(df, key_cols)
    dedup_path = os.path.join(Config.OUTPUT_PATH, Config.FILE_DEDUP)
    df_final.to_csv(dedup_path, index=False, encoding='utf-8-sig')
    with open(os.path.join(Config.OUTPUT_PATH, Config.FILE_MANUAL_REVIEW), 'w', encoding='utf-8') as f:
        json.dump(manual_review, f, ensure_ascii=False, indent=2)
    logging.info(f"å»é‡çµæœå·²å„²å­˜ï¼š{dedup_path}")
    return df_final, manual_review

dedup_df, manual_review = perform_deduplication(assessed_df)

# ------------------------------
# STEP 7ï¼šå ±å‘Šç”Ÿæˆï¼ˆExcelã€è‹±æ–‡ PDFã€é©—è­‰æŒ‡å—ï¼‰
# ------------------------------
class ReportGenerator:
    """ç”¢ç”Ÿ Excel èˆ‡ PDFï¼ˆPDF ç‚ºè‹±æ–‡æ®µè½æ‘˜è¦ï¼‰"""
    @staticmethod
    def summary_stats(df: pd.DataFrame) -> Dict:
        stats = {
            'total_companies': len(df),
            'real_companies': int(df['is_real_company'].sum()) if 'is_real_company' in df.columns else 0,
            'unique_companies': int(df['company_name_clean'].nunique()),
            'high_risk_count': int((df['risk_score'] >= 70).sum()),
            'lei_coverage': int(df['lei_enhanced_valid'].sum()) if 'lei_enhanced_valid' in df.columns else int(df['lei_enhanced'].notna().sum()),
            'ofac_hits': int(df['ofac_hit'].sum()) if 'ofac_hit' in df.columns else 0,
            'kyc_expired': int(df['kyc_refresh_due'].sum())
        }
        return stats

    @staticmethod
    def generate_excel(df: pd.DataFrame, outdir: str) -> str:
        out = os.path.join(outdir, Config.FILE_EXCEL)
        with pd.ExcelWriter(out, engine='openpyxl') as writer:
            cols = [c for c in ['record_id','company_name','company_name_clean','registered_country','reg_no','reg_no_normalized','risk_score','risk_category','lei_enhanced','lei_enhanced_valid','ofac_hit','ofac_confidence','kyc_refresh_due','days_until_next_review','aum_usd_millions','incorporation_date','last_kyc_review'] if c in df.columns]
            df[cols].to_excel(writer, sheet_name='Customer Risk Overview', index=False)
            high = df[df['risk_score'] >= 70]
            if not high.empty:
                high[cols].to_excel(writer, sheet_name='High Risk Customers', index=False)
            if 'ofac_hit' in df.columns:
                sanctions = df[df['ofac_hit'] == True]
                if not sanctions.empty:
                    scols = [c for c in ['record_id','company_name_clean','registered_country','ofac_hit','ofac_confidence','ofac_matched_entity','risk_score'] if c in df.columns]
                    sanctions[scols].to_excel(writer, sheet_name='Sanctions Hits', index=False)
            stats = ReportGenerator.summary_stats(df)
            stats_df = pd.DataFrame([{'Metric':'Total Companies','Value':stats['total_companies']},
                                     {'Metric':'LEI Coverage','Value':stats['lei_coverage']},
                                     {'Metric':'Sanctions Hits','Value':stats['ofac_hits']},
                                     {'Metric':'KYC Overdue','Value':stats['kyc_expired']},
                                     {'Metric':'Generated At','Value':datetime.now().strftime('%Y-%m-%d %H:%M:%S')}])
            stats_df.to_excel(writer, sheet_name='Summary', index=False)
        logging.info(f"Excel å·²ç”Ÿæˆï¼š{out}")
        return out

    @staticmethod
    def generate_pdf(df: pd.DataFrame, outdir: str) -> str:
        """
        ç”Ÿæˆè‹±æ–‡å ±å‘Šï¼ˆåªæœ‰æ–‡å­—æ®µè½ï¼Œä¸å«è¡¨æ ¼ï¼‰ã€‚
        æ¨™é¡Œä½¿ç”¨æ­£å¼éŠ€è¡Œæ ¼å¼ï¼š "KYC Compliance Summary Report"
        """
        out = os.path.join(outdir, Config.FILE_PDF)
        doc = SimpleDocTemplate(out, pagesize=letter)
        styles = getSampleStyleSheet()
        story = []

        # ä»¥å…§å»ºè‹±æ–‡å­—å‹ï¼ˆHelvetica ç³»åˆ—ï¼‰ç”¢ç”Ÿæ¨£å¼
        title_style = ParagraphStyle(name='TitleEN', parent=styles['Title'], fontName='Helvetica-Bold', fontSize=18)
        normal_style = ParagraphStyle(name='NormalEN', parent=styles['Normal'], fontName='Helvetica', fontSize=10, leading=14)

        # è‹±æ–‡æ¨™é¡Œï¼ˆæ­£å¼éŠ€è¡Œæ ¼å¼ï¼‰
        title = Paragraph("KYC Compliance Summary Report", title_style)
        story.append(title)
        story.append(Spacer(1, 12))

        # è‹±æ–‡æ®µè½æ‘˜è¦ï¼ˆç°¡æ½”ã€é©åˆçµ¦ç®¡ç†å±¤ï¼‰
        stats = ReportGenerator.summary_stats(df)
        p1 = f"This report provides a concise summary of the KYC dataset processed by the v16 B version KYC compliance system."
        p2 = f"Total companies processed: {stats['total_companies']}. LEI coverage (validated): {stats['lei_coverage']}. Sanctions hits detected: {stats['ofac_hits']}."
        p3 = f"KYC overdue cases: {stats['kyc_expired']}. High-risk customers (risk score >=70): {stats['high_risk_count']}."
        p4 = "Notes: LEI enrichment attempted via GLEIF queries for all candidate names; OFAC screening performed against available SDN data (local copy or online source). Duplicate detection used MinHash LSH with dynamic weights influenced by risk profiles."

        story.append(Paragraph(p1, normal_style)); story.append(Spacer(1,8))
        story.append(Paragraph(p2, normal_style)); story.append(Spacer(1,8))
        story.append(Paragraph(p3, normal_style)); story.append(Spacer(1,8))
        story.append(Paragraph(p4, normal_style)); story.append(Spacer(1,12))

        ts = f"Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        story.append(Paragraph(ts, normal_style))

        doc.build(story)
        logging.info(f"PDF å·²ç”Ÿæˆï¼š{out}")
        return out

    @staticmethod
    def generate_verification_guide(outdir: str) -> str:
        out = os.path.join(outdir, Config.FILE_GUIDE)
        with open(out, 'w', encoding='utf-8') as f:
            f.write("KYC System v16 Verification Guide\n")
            f.write(f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            f.write("Check the following items:\n")
            f.write("- LEI enrichment coverage and validity\n")
            f.write("- OFAC/SDN parsing and matching\n")
            f.write("- Deduplication output and manual review notes\n")
            f.write("- Risk distribution summary\n")
        logging.info(f"Verification guide saved: {out}")
        return out

# ç”¢ç”Ÿå ±è¡¨
excel_path = ReportGenerator.generate_excel(dedup_df, Config.OUTPUT_PATH)
pdf_path = ReportGenerator.generate_pdf(dedup_df, Config.OUTPUT_PATH)
guide_path = ReportGenerator.generate_verification_guide(Config.OUTPUT_PATH)

# ------------------------------
# STEP 8ï¼šè¼¸å‡ºæª”æ¡ˆåˆ—èˆ‰èˆ‡ç³»çµ±é©—è­‰æ‘˜è¦
# ------------------------------
def list_output_files():
    print("\nå·²è¼¸å‡ºæª”æ¡ˆä½æ–¼ï¼š", Config.OUTPUT_PATH)
    for fn in sorted(os.listdir(Config.OUTPUT_PATH)):
        path = os.path.join(Config.OUTPUT_PATH, fn)
        try:
            size_kb = os.path.getsize(path) / 1024
            print(f" - {fn} ({size_kb:.1f} KB)")
        except:
            print(f" - {fn}")

list_output_files()

def system_validation(raw_df, final_df):
    print("\nç³»çµ±é©—è­‰æ‘˜è¦ï¼š")
    total = len(final_df)
    missing_names = final_df['company_name_clean'].isna().sum()
    print(f" - è™•ç†è¨˜éŒ„æ•¸ï¼š{total}")
    print(f" - ç¼ºå¤±å…¬å¸åï¼š{missing_names}")
    lei_total = final_df['lei_enhanced'].notna().sum() if 'lei_enhanced' in final_df.columns else 0
    lei_valid = final_df['lei_enhanced_valid'].sum() if 'lei_enhanced_valid' in final_df.columns else 0
    lei_pct = (lei_valid / total * 100) if total > 0 else 0
    print(f" - LEI è¦†è“‹ï¼ˆé©—è­‰é€šéï¼‰ï¼š{lei_valid}/{lei_total} ({lei_pct:.2f}%)")
    ofac_hits = final_df['ofac_hit'].sum() if 'ofac_hit' in final_df.columns else 0
    print(f" - OFAC å‘½ä¸­ï¼š{ofac_hits} ({ofac_hits/total*100:.3f}%)")
    kyc_expired = final_df['kyc_refresh_due'].sum()
    print(f" - KYC éæœŸï¼š{kyc_expired} ({kyc_expired/total*100:.1f}%)")
    dup_test_file = os.path.join(Config.OUTPUT_PATH, Config.FILE_DUP_TEST)
    print(f" - å»é‡çµ„æ•¸é‡ï¼ˆæ¸¬è©¦ç’°å¢ƒæ¸…å–®ï¼‰ï¼š{dup_test_file if os.path.exists(dup_test_file) else 'ç„¡'}")

system_validation(raw_df, dedup_df)

print("\nå®Œæˆï¼šKYC ç³»çµ±åŸ·è¡ŒçµæŸã€‚")

